{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "4ae3e0c7-eb15-4da8-a604-b2c111895e1c",
   "metadata": {
    "language": "python",
    "name": "cell38",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "import streamlit as st",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61afc700-d7f3-4905-a4e4-d53be0248d9b",
   "metadata": {
    "name": "cell26",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Some LLMs are only deployed within specific AWS regions.  To use LLMs deployed in regions other than that of your Snowflake account, you can enable cross-region inference with the followign command:"
  },
  {
   "cell_type": "code",
   "id": "272357d8-e47d-45bc-b468-405613386ae4",
   "metadata": {
    "language": "sql",
    "name": "cell28",
    "resultHeight": 111,
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ALTER ACCOUNT SET CORTEX_ENABLED_CROSS_REGION = 'AWS_US'",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c84a2c0-ea96-4b15-b8d2-7eeb43f374c8",
   "metadata": {
    "name": "cell2",
    "collapsed": false,
    "resultHeight": 112
   },
   "source": "# Build a Retrieval Augmented Generation (RAG) based LLM assistant using Streamlit and Snowflake Cortex"
  },
  {
   "cell_type": "markdown",
   "id": "9af5ea6c-58a9-44ae-ace8-fb6881a25253",
   "metadata": {
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 596
   },
   "source": "## 1. Overview\n\nTo reduce hallucinations (i.e. incorrect responses), LLMs can be combined with private datasets. Today, the most common approach for reducing hallucinations without having to change the model (e.g. fine-tuning) is the Retrieval Augmented Generation (RAG) framework. RAG allows you to \"ground\" the model's responses by making a set of relevant documents available to the LLM as context in the response.\n\nIn this lab, we will show you how to quickly and securely build a full-stack RAG application in Snowflake without having to build integrations, manage any infrastructure, or deal with security concerns with data moving outside of the Snowflake governance framework.\n\nWe will show you how easy it is to implement RAG via a chat assistant that knows everything about bicycles. This assistant can be really useful for your adventure-seeking friend or relative who may be interested in all things bike-related. To make the assistant an expert in a bicycles, we are going to give it access to a few User Manuals. This template can easily be adapted to other documents that may be more interesting to you whether its financial reports, research documents, or anything else!\n\nAlong the way, we will also share tips on how you could turn what may seem like a prototype into a production pipeline by showing you how to automatically process new documents as they are uploaded as well as learn about relevant Snowflake functionality to consider for additional enhancements.\n\n### What You Will Build\n\nThe final product includes an application that lets users test how the LLM responds with and without the context document(s) to show how RAG can address hallucinations."
  },
  {
   "cell_type": "code",
   "id": "c09fd90d-9a44-4585-834a-5ca19b44c0bb",
   "metadata": {
    "language": "python",
    "name": "cell39",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 620
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/667a3b85e3f70372.gif', width = 750)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "543bb75b-c97c-417f-be29-07439d03ee69",
   "metadata": {
    "name": "cell41",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### RAG Overview"
  },
  {
   "cell_type": "code",
   "id": "614fd65f-dd8f-4a2b-aa7c-cea8c9c5088d",
   "metadata": {
    "language": "python",
    "name": "cell44",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 346
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/883cc4b7c866c16d.png', width = 750)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4914aa27-1197-41d1-b0f5-39e924d1a657",
   "metadata": {
    "name": "cell40",
    "collapsed": false,
    "resultHeight": 425
   },
   "source": "### What You Will Learn\n\nHow to create analytical processing queries in Snowflake Notebooks\n\nHow to generate embeddings, run semantic search, and use LLMs using serverless functions in [Snowflake Cortex](https://www.snowflake.com/cortex?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)\n\nHow to build a front-end with Python using [Streamlit in Snowflake](https://www.snowflake.com/en/data-cloud/overview/streamlit-in-snowflake/?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)\n\nOptional: How to automate data processing pipelines using directory tables, [Streams](https://docs.snowflake.com/en/user-guide/streams-intro?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O) and [Task](https://docs.snowflake.com/en/user-guide/tasks-intro?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)\n\n### Prerequisites\n\nSnowflake account in a cloud region where Snowflake Cortex LLM functions are supported\n\nCheck [LLM availability](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions?_ga=2.5151286.405859672.1709568467-277021311.1701887324&_gac=1.124754680.1707955750.Cj0KCQiA5rGuBhCnARIsAN11vgRLWfK6RIoIEqcZ7cFas8qwN4yCoL0q9nttp5UEmSocnPmhdBG57fgaAjqNEALw_wcB&_fsi=j2b82Wl3&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O#availability) to help you decide where you want to create your snowflake account\n\nA Snowflake account with [Anaconda Packages](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-packages?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O#using-third-party-packages-from-anaconda) enabled by ORGADMIN.\n\nSnowflake Cortex vector functions for semantic distance calculations along with VECTOR as a data type enabled."
  },
  {
   "cell_type": "markdown",
   "id": "5ed6821f-4797-46cb-a66d-cdfc9c555508",
   "metadata": {
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 448
   },
   "source": "## 2. Organize Documents\n\nIn Snowflake, databases and schemas are used to organize and govern access to data and logic. LetÂ´s start by getting a few documents locally and then create a database that will hold the PDFs, the queries that will process (extract and chunk) those PDFs, and the table that will hold the text embeddings.\n\n### Step 1. Download example documents\n\nLet's download a few documents we have created about bikes. In those documents, we have added some very specific information about those ficticious models. You can always add more or use a different type of documents that you want to try asking questions against. At the end, we are going to test how the LLM responds with and without access to the information in the documents:\n\n- [Bicycle Documents](https://github.com/phdata/snowflake-cortex-rag-assistant-lab/tree/main/bicycle-documents)\n\n### Step 2. Create a database and a schema\n\nRun the following code inside your newly created worksheet:"
  },
  {
   "cell_type": "code",
   "id": "6a9bf230-61e5-4e8c-81af-95aaa75c2876",
   "metadata": {
    "language": "sql",
    "name": "cell4",
    "codeCollapsed": false,
    "resultHeight": 111,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DATABASE HOL_CORTEX_DOCS;\nCREATE OR REPLACE SCHEMA DATA;\nUSE HOL_CORTEX_DOCS.DATA;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dcdf399d-fd46-4b4a-bcea-5f94b28c94fb",
   "metadata": {
    "name": "cell7",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Step 3. Create a Stage with Directory Table where you will be uploading your documents:"
  },
  {
   "cell_type": "code",
   "id": "6117acdb-9dec-45f5-889c-d19c83c3206a",
   "metadata": {
    "language": "sql",
    "name": "cell8",
    "codeCollapsed": false,
    "resultHeight": 111,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE STAGE docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51f880c4-6236-42d6-bc64-992a43d6bffb",
   "metadata": {
    "name": "cell9",
    "collapsed": false,
    "resultHeight": 299
   },
   "source": "### Step 4. Upload documents to your staging area\n\n- Select Data on the left of Snowsight\n\n- Click on your database HOL_CORTEX_DOCS\n\n- Click on your schema DATA\n\n- Click on Stages and select DOCS\n\n- On the top right click on the +Files botton\n\n- Drag and drop the four PDF files you downloaded\n"
  },
  {
   "cell_type": "code",
   "id": "03500d55-149a-4328-ac9e-f851ffeca52e",
   "metadata": {
    "language": "python",
    "name": "cell46",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 305
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/4ebe360bbabfc9e0.png', width = 750)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df252299-535f-452b-a76d-c415c48198b6",
   "metadata": {
    "name": "cell45",
    "collapsed": false,
    "resultHeight": 88
   },
   "source": "### Step 5. Check files has been successfully uploaded\n\nRun this query to check what documents are in the staging area:"
  },
  {
   "cell_type": "code",
   "id": "64861c4d-5e20-49e2-bb3d-695c3f411c93",
   "metadata": {
    "language": "sql",
    "name": "cell10",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 251
   },
   "outputs": [],
   "source": "ls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81801c0d-9b7a-4d6e-a04e-6979fc730287",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 182
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/41c2a99da0ea5784.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23ae02ea-9224-4369-ba3b-03b06e338ea4",
   "metadata": {
    "name": "cell12",
    "collapsed": false,
    "resultHeight": 153
   },
   "source": "## 3. Build the Vector Store\n\nIn this step, we are going to leverage our document processing functions to prepare documents before turning the text into embeddings using Snowflake Cortex. These embeddings will be stored in a Snowflake Table using the new native VECTOR data type."
  },
  {
   "cell_type": "code",
   "id": "c10cf524-57bc-427c-9520-8404e4e323b4",
   "metadata": {
    "language": "python",
    "name": "cell47",
    "collapsed": false,
    "resultHeight": 188,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/f1b8d45a406f28a1.png', width = 750)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a0b4d6e-ceae-461b-8a1b-1ca6cee548d2",
   "metadata": {
    "name": "cell49",
    "collapsed": false,
    "resultHeight": 88
   },
   "source": "### Step 1. Create the table where we are going to store the chunks and vectors for each PDF. \n\nNote here the usage of the new VECTOR data type:"
  },
  {
   "cell_type": "code",
   "id": "0e57f79d-9465-4791-94fe-b55aaa55dae6",
   "metadata": {
    "language": "sql",
    "name": "cell13",
    "codeCollapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE DOCS_CHUNKS_TABLE ( \n    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n    SIZE NUMBER(38,0), -- Size of the PDF\n    FILE_URL VARCHAR(16777216), -- URL for the PDF\n    CHUNK ARRAY, -- Piece of text\n    CHUNK_VEC VECTOR(FLOAT, 768) );  -- Embedding using the VECTOR data type",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a53c943c-a96a-4173-9161-68c1e5c6a5ec",
   "metadata": {
    "name": "cell5",
    "collapsed": false,
    "resultHeight": 241
   },
   "source": "Relevant documentation: [Database and Schema management](https://docs.snowflake.com/en/sql-reference/ddl-database?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)\n\n### Step 2. Create a query that will read the PDF documents and split them in chunks.\n\nProcess the PDF files, extract the chunks, and create the embeddings. Insert that info in the table we have just created.\n\nRelevant documentation:\n\n- [PARSE_DOCUMENT (SNOWFLAKE.CORTEX)](https://docs.snowflake.com/en/sql-reference/functions/parse_document-snowflake-cortex)\n- [SPLIT_TEXT_RECURSIVE_CHARACTER (SNOWFLAKE.CORTEX)](https://docs.snowflake.com/sql-reference/functions/split_text_recursive_character-snowflake-cortex)"
  },
  {
   "cell_type": "code",
   "id": "528b97a5-3b88-4341-8aa4-32795d94ceba",
   "metadata": {
    "language": "sql",
    "name": "cell31",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "INSERT INTO docs_chunks_table (relative_path, size, file_url, chunk, chunk_vec)\nSELECT\n    relative_path, \n    size,\n    file_url, \n    split_table.chunks AS chunk,\n    SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', chunk[0]) AS chunk_vec\nFROM (\n    SELECT relative_path, size, file_url, SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(parsed_table.parsed:content, 'none', 4000, 400) AS chunks\n    FROM (\n        SELECT relative_path, size, file_url, SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@docs', relative_path, { 'mode': 'OCR' }) as parsed\n        FROM directory(@docs)\n    ) AS parsed_table\n) AS split_table;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3756d4b2-eaec-4997-838f-6c6eaa5f6059",
   "metadata": {
    "name": "cell16",
    "collapsed": false,
    "resultHeight": 305
   },
   "source": "### Explanation of the previous code:\n\nThe code above contains a nested query, which first, using Snowflake Cortex PARSE_DOCUMENT, returns the extracted content from the documents in a Snowflake stage, as an OBJECT that contains JSON-encoded objects as strings. The Snowflake Cortex SPLIT_TEXT_RECURSIVE_CHARACTER function splits a string into shorter stings, recursively, for preprocessing text to be used with text embedding or search indexing functions. These two functions, in combination, produce the chunks that are the pieces of text from the PDFs.\n\nThe chunk text is passed again to Snowflake Cortex to generate the embeddings with this code:\n\n            SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2',chunk) as chunk_vec"
  },
  {
   "cell_type": "markdown",
   "id": "99e737d3-e701-4ff3-aa0e-e729b2bf7656",
   "metadata": {
    "name": "cell19",
    "collapsed": false,
    "resultHeight": 83
   },
   "source": "That code is calling the embed_text function using the e5-base-v2 transformer and returning an embedding vector.\n\nIf you check the docs_chunks_table table, you should see the PDFs has been processed"
  },
  {
   "cell_type": "code",
   "id": "a8426cc3-189c-4330-8b29-4fe6b29c4c72",
   "metadata": {
    "language": "sql",
    "name": "cell18",
    "resultHeight": 216
   },
   "outputs": [],
   "source": "select relative_path, size, chunk, chunk_vec from docs_chunks_table limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57ddbcd9-b296-4c6f-93a7-7f973eabadf6",
   "metadata": {
    "language": "python",
    "name": "cell50",
    "codeCollapsed": false,
    "resultHeight": 204
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/a0e9299a0aaf3a84.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a31ead1b-85a4-4959-8cdb-f97d342c6ba2",
   "metadata": {
    "name": "cell20",
    "collapsed": false,
    "resultHeight": 108
   },
   "source": "And you can see that the CHUNK_VEC columns contain the embedings using the VECTOR data type.\n\nYour PDF files have been chunked, and each chunk has an embedding. We can check how many chunks we got for each file using this query"
  },
  {
   "cell_type": "code",
   "id": "f1292774-8e8d-4d35-a2af-707fae57224d",
   "metadata": {
    "language": "sql",
    "name": "cell21",
    "codeCollapsed": false,
    "resultHeight": 216
   },
   "outputs": [],
   "source": "select relative_path, ARRAY_SIZE(chunk) as num_chunks \nfrom docs_chunks_table;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91fee5d9-72d4-4076-91fc-051b0bc09240",
   "metadata": {
    "language": "python",
    "name": "cell51",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 182
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/de9dc723d11d0beb.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4dca0e18-4699-442d-874a-b95feddee1c0",
   "metadata": {
    "name": "cell22",
    "collapsed": false,
    "resultHeight": 111
   },
   "source": "After completing all the steps in this section, you should see the following objects in your database:\n\n- The DOCS_CHUNKS_TABLE that contains the text and the vectors\n- The DOCS Stage with your PDF files"
  },
  {
   "cell_type": "code",
   "id": "93d08c3b-57a2-4eed-98a4-286e5b41c3f4",
   "metadata": {
    "language": "python",
    "name": "cell52",
    "collapsed": false,
    "resultHeight": 266
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/d66a6177dc1583dd.png', width = 250)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21d3a838-37c6-42cb-9b7b-3b26446787d3",
   "metadata": {
    "name": "cell23",
    "collapsed": false,
    "resultHeight": 419
   },
   "source": "## 4. Build Chat UI and Chat (Retrieval and Generation) Logic\n\nTo make it easy for anyone to ask questions against the vector store, let's create a fairly simple front-end using Streamlit. As part of the app, we will provide the end-user with a toggle that allows testing of LLM responses with and without access to the context to observe the differences.\n\nStreamlit in Snowflake allows you to run the app and share it with other Snowflake users within the same account. This ensures data remains secure and protected and is only available to users that meet your role-based access policies.\n\nRelevant documentation: [Introduction to Streamlit in Snowflake](https://docs.snowflake.com/en/developer-guide/streamlit/about-streamlit?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)\n\n- Click on the Streamlit tab on the left\n- Click on + Streamlit App button on the right\n- Give the App a name (HOL_CORTEX_APP in my example)\n- Select the warehouse to run the App (a Small WH will be enough)\n- Choose the HOL_CORTEX_DOCS database and DATA schema"
  },
  {
   "cell_type": "code",
   "id": "58a404c2-6179-45c4-b4b7-6664cc89ec95",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/6fed480e8ea7c5d7.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a181d82f-b3f5-423e-a272-c1e5365bca58",
   "metadata": {
    "name": "cell48",
    "collapsed": false,
    "resultHeight": 278
   },
   "source": "The Streamlit app comes with a default template you can delete and replace with this code which includes the front-end elements:\n\n- Table with list of available documents\n- Toggle to choose to use documents as context\n- Question input box\n\nAnd also includes the retrieval and generation logic:\n\n- Function to calculate distance between question and text chunks to retrieve top result\n- Prompt template that instructs LLM how to answer using relevant chunks"
  },
  {
   "cell_type": "code",
   "id": "f959508e-8ed2-46f3-a409-353a479ff8be",
   "metadata": {
    "language": "python",
    "name": "cell24",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "import streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session() # Get the current credentials\n\nimport pandas as pd\n\npd.set_option(\"max_colwidth\",None)\nnum_chunks = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy\n\ndef create_prompt (myquestion, rag):\n\n    if rag == 1:    \n\n        cmd = \"\"\"\n         with results as\n         (SELECT RELATIVE_PATH,\n           VECTOR_COSINE_SIMILARITY(docs_chunks_table.chunk_vec,\n                    SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', ?)) as similarity,\n           chunk\n         from docs_chunks_table\n         order by similarity desc\n         limit ?)\n         select chunk, relative_path from results \n         \"\"\"\n    \n        df_context = session.sql(cmd, params=[myquestion, num_chunks]).to_pandas()      \n        \n        context_lenght = len(df_context) -1\n\n        prompt_context = \"\"\n        for i in range (0, context_lenght):\n            prompt_context += df_context._get_value(i, 'CHUNK')\n\n        prompt_context = prompt_context.replace(\"'\", \"\")\n        relative_path =  df_context._get_value(0,'RELATIVE_PATH')\n    \n        prompt = f\"\"\"\n          'You are an expert assistance extracting information from context provided. \n           Answer the question based on the context. Be concise and do not hallucinate. \n           If you donÂ´t have the information just say so.\n          Context: {prompt_context}\n          Question:  \n           {myquestion} \n           Answer: '\n           \"\"\"\n        cmd2 = f\"select GET_PRESIGNED_URL(@docs, '{relative_path}', 360) as URL_LINK from directory(@docs)\"\n        df_url_link = session.sql(cmd2).to_pandas()\n        url_link = df_url_link._get_value(0,'URL_LINK')\n\n    else:\n        prompt = f\"\"\"\n         'Question:  \n           {myquestion} \n           Answer: '\n           \"\"\"\n        url_link = \"None\"\n        relative_path = \"None\"\n        \n    return prompt, url_link, relative_path\n\ndef complete(myquestion, model_name, rag = 1):\n\n    prompt, url_link, relative_path =create_prompt (myquestion, rag)\n    cmd = f\"\"\"\n             select SNOWFLAKE.CORTEX.COMPLETE(?,?) as response\n           \"\"\"\n    \n    df_response = session.sql(cmd, params=[model_name, prompt]).collect()\n    return df_response, url_link, relative_path\n\ndef display_response (question, model, rag=0):\n    response, url_link, relative_path = complete(question, model, rag)\n    res_text = response[0].RESPONSE\n    st.markdown(res_text)\n    if rag == 1:\n        display_url = f\"Link to [{relative_path}]({url_link}) that may be useful\"\n        st.markdown(display_url)\n\n#Main code\n\nst.title(\"Asking Questions to Your Own Documents with Snowflake Cortex:\")\nst.write(\"\"\"You can ask questions and decide if you want to use your documents for context or allow the model to create their own response.\"\"\")\nst.write(\"This is the list of documents you already have:\")\ndocs_available = session.sql(\"ls @docs\").collect()\nlist_docs = []\nfor doc in docs_available:\n    list_docs.append(doc[\"name\"])\nst.dataframe(list_docs)\n\n#Here you can choose what LLM to use. Please note that they will have different cost & performance\nmodel = st.sidebar.selectbox('Select your model:',(\n                                    'mixtral-8x7b',\n                                    'snowflake-arctic',\n                                    'mistral-large',\n                                    'llama3-8b',\n                                    'llama3-70b',\n                                    'reka-flash',\n                                     'mistral-7b',\n                                     'llama2-70b-chat',\n                                     'gemma-7b'))\n\nquestion = st.text_input(\"Enter question\", placeholder=\"Is there any special lubricant to be used with the premium bike?\", label_visibility=\"collapsed\")\n\nrag = st.sidebar.checkbox('Use your own documents as context?')\n\nprint (rag)\n\nif rag:\n    use_rag = 1\nelse:\n    use_rag = 0\n\nif question:\n    display_response (question, model, use_rag)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce5992dc-f7cb-4030-ab91-cb857af4efb3",
   "metadata": {
    "name": "cell25",
    "collapsed": false,
    "resultHeight": 654
   },
   "source": "### Explanation\n\nLetÂ´s go step by step what that code is doing:\n\ncreate_prompt() receives a question as an argument and whether it has to use the context documents or not. This can be used to compare how the LLM responds when using the RAG framework vs. using existing knowledge gained during pre-training.\n\nWhen the box is checked, this code is going embed the question and look for the PDF chunk with the closest similarity to the question being asked. We can limit the number of chunks we want to provide as a context, and that text will be added to the prompt.\n\n        cmd = \"\"\"\n          with results as\n         (SELECT RELATIVE_PATH,\n           VECTOR_COSINE_SIMILARITY(docs_chunks_table.chunk_vec,\n                    SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', ?)) as similarity,\n           chunk\n         from docs_chunks_table\n         order by similarity desc\n         limit ?)\n         select chunk, relative_path from results \n\n          \"\"\"\n    \n    df_context = session.sql(cmd, params=[myquestion, num_chunks]).to_pandas()"
  },
  {
   "cell_type": "markdown",
   "id": "9f97015a-d681-4b5f-877a-0c58ea3a8f12",
   "metadata": {
    "name": "cell27",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "So the code is doing a similarity search to look for the closest chunk of text and provide it as context in the prompt:"
  },
  {
   "cell_type": "code",
   "id": "95fb646a-2a5d-4615-83c7-fce0aedcca69",
   "metadata": {
    "language": "python",
    "name": "cell55",
    "collapsed": false,
    "resultHeight": 97
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/39c79ba46e7fd04.png', width = 750)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc784457-23be-40fa-b868-811a4b80b126",
   "metadata": {
    "name": "cell54",
    "collapsed": false,
    "resultHeight": 371
   },
   "source": "The next section worth calling out is the complete() function which combines the LLM, the prompt template and whether to use the context or not to generate a response, which includes a link to the asset from which the answer was obtained for the user to verify the results.\n\n    def complete(myquestion, model_name, rag = 1):\n\n    prompt, url_link, relative_path =create_prompt (myquestion, rag)\n    cmd = f\"\"\"\n             select SNOWFLAKE.CORTEX.COMPLETE(?,?) as response\n           \"\"\"\n    \n    df_response = session.sql(cmd, params=[model_name, prompt]).collect()\n    return df_response, url_link, relative_path"
  },
  {
   "cell_type": "code",
   "id": "58ee8076-6e43-4017-aeca-d4fcd47a9193",
   "metadata": {
    "language": "python",
    "name": "cell56",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 124
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/4f13b8a35b4f9c16.png', width = 750)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "983c8600-e8d5-447b-a927-295c1355f4b9",
   "metadata": {
    "name": "cell29",
    "collapsed": false,
    "resultHeight": 113
   },
   "source": "### Deploy and share your AI-powered app\n\nStreamlit in Snowflake provides a side-by-side editor and preview screen that makes it easy and fast to iterate and visalize changes."
  },
  {
   "cell_type": "code",
   "id": "1757e078-2f2a-4541-bf1a-8da34a22ae7c",
   "metadata": {
    "language": "python",
    "name": "cell58",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 636
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/c52432a94c8e830f.png', width = 750)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ba4c146-d739-4f5c-a97f-2e17897c0597",
   "metadata": {
    "name": "cell57",
    "collapsed": false,
    "resultHeight": 1171
   },
   "source": "In the app, we can see the documents we had uploaded previously and can be used to ask questions while trying multiple options using interactive widgets:\n\n- LLM dropdown: Evaluate the response to the same question from different LLMs available in Snowflake Cortex.\n- Context toggle: Check the box to receive answer with RAG. Uncheck to see how LLM answers without access to the context.\n\nTo test out the RAG framework, here a few questions you can ask and then use the interactive widgets to compare the results when using a different LLM or when choosing to get a response without the context. This is related to very specific information that we have added into the documents and that is very unique to our products.\n\n- Is there any special lubricant to be used with the premium bike?\n- What is the warranty for the premium bike?\n- Does the mondracer infant bike need any special tool?\n- Is there any temperature to be considered with the premium bicycle?\n- What is the temperature to store the ski boots?\n- Where have the ski boots been tested and who tested them?\n\n### Other things to test\n\nIn this example we have just decided a fixed format for chunks and used only the top result in the retrieval process. This [blog](https://medium.com/@thechosentom/rag-made-simple-with-snowflake-cortex-74d1df5143fd) provides some considerations about settings in the chunking function in our Snowpark UDTF.\n\n    text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 4000, #Adjust this as you see fit\n            chunk_overlap  = 400, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n    )\n\nYou can also try different instructions in your prompt and see how the responses may vary. Simply replace any of the text and run the app again.\n\n    prompt = f\"\"\"\n          'You are an expert assistance extracting information from context provided. \n           Answer the question based on the context. Be concise and do not hallucinate. \n           If you donÂ´t have the information just say so.\n          Context: {prompt_context}\n          Question:  \n           {myquestion} \n           Answer: '\n           \"\"\"\n\nYou can also try to change the number of chunks that are provided as context by simply modifying this value:\n\n    num_chunks = 3"
  },
  {
   "cell_type": "markdown",
   "id": "91de472c-7979-4498-8f6a-a460421c8e1e",
   "metadata": {
    "name": "cell35",
    "collapsed": false,
    "resultHeight": 575
   },
   "source": "## 5. Build a ChatBot UI that Remember Previous Conversations\n\nIn the previous section we have created a simple interface where we can ask questions about our documents and select the LLM running within Snowflake Cortex to answer the question. We have seen that when no context from our documents is provided, we just get a general answer, versus a specific answer related to our documents when we use context from the PDFs. But what if we want to have a conversation style?\n\nStreamlit makes it very easy to create chat interfaces with its [Chat Elements](https://docs.streamlit.io/develop/api-reference/chat?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O&_fsi=P4Dy8V0O). In this section we are going to see how we can write a simple app that can be used to have a conversation, and how it uses the data from our documents to provide answers.\n\nLarge Language Models (LLMs) are stateless. That means when we make a call to any of them, it does not remember what the previous call was about. With Snowflake Cortex, each complete() call we make is independent, even when using the same model. There are several techniques that can be used to keep the conversation flow and do not forget the chat history. In summary, the app needs to keep a memory of the conversation and provide it in each call to the LLM. As we know, calls to LLMs are restricted by the context window length that can be used.\n\nIn this lab, we are going to use a slide window concept. This helps to remember just a number of past interactions when calling the LLM. We are also going to summarize the previous conversation in order to find the right chunk in our documents that will help the LLM and provide the right answer. In the previous section, we were embedding the new question to find the right chunk for context. Here, we are going to also include the previous summary to find the chunks that will be used as context.\n\nFirst letÂ´s create the new Streamlit App and then we will discuss each of the steps. Give it a name and create it within the database and schema that we are using in this lab (HOL_CORTEX_DOCS.DATA) or whatever name you may have chosen."
  },
  {
   "cell_type": "code",
   "id": "9627371d-b414-4d62-b9e8-8526685978eb",
   "metadata": {
    "language": "python",
    "name": "cell60",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 377
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/965108e06f8ff2ba.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f293f79-6e23-4be1-83b6-4cfbeaeaf03d",
   "metadata": {
    "name": "cell59",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "The Streamlit app comes with a default template you can delete and replace with this code which includes the front-end elements:"
  },
  {
   "cell_type": "code",
   "id": "7dd2cac0-396a-45e2-b3d1-f2705fc03f41",
   "metadata": {
    "language": "python",
    "name": "cell36",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "import streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session() # Get the current credentials\n\nimport pandas as pd\n\npd.set_option(\"max_colwidth\",None)\n\n### Default Values\n#model_name = 'mistral-7b' #Default but we allow user to select one\nnum_chunks = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy\nslide_window = 7 # how many last conversations to remember. This is the slide window.\n#debug = 1 #Set this to 1 if you want to see what is the text created as summary and sent to get chunks\n#use_chat_history = 0 #Use the chat history by default\n\n### Functions\n\ndef main():\n    \n    st.title(f\":speech_balloon: Chat Document Assistant with Snowflake Cortex\")\n    st.write(\"This is the list of documents you already have and that will be used to answer your questions:\")\n    docs_available = session.sql(\"ls @docs\").collect()\n    list_docs = []\n    for doc in docs_available:\n        list_docs.append(doc[\"name\"])\n    st.dataframe(list_docs)\n\n    config_options()\n    init_messages()\n     \n    # Display chat messages from history on app rerun\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n    \n    # Accept user input\n    if question := st.chat_input(\"What do you want to know about your products?\"):\n        # Add user message to chat history\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n        # Display user message in chat message container\n        with st.chat_message(\"user\"):\n            st.markdown(question)\n        # Display assistant response in chat message container\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()\n    \n            question = question.replace(\"'\",\"\")\n    \n            with st.spinner(f\"{st.session_state.model_name} thinking...\"):\n                response = complete(question)\n                res_text = response[0].RESPONSE     \n            \n                res_text = res_text.replace(\"'\", \"\")\n                message_placeholder.markdown(res_text)\n        \n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": res_text})\n\n\ndef config_options():\n\n\n    \n    st.sidebar.selectbox('Select your model:',(\n                                    'mixtral-8x7b',\n                                    'snowflake-arctic',\n                                    'mistral-large',\n                                    'llama3-8b',\n                                    'llama3-70b',\n                                    'reka-flash',\n                                     'mistral-7b',\n                                     'llama2-70b-chat',\n                                     'gemma-7b'), key=\"model_name\")\n                                           \n    # For educational purposes. Users can chech the difference when using memory or not\n    st.sidebar.checkbox('Do you want that I remember the chat history?', key=\"use_chat_history\", value = True)\n\n    st.sidebar.checkbox('Debug: Click to see summary generated of previous conversation', key=\"debug\", value = True)\n    st.sidebar.button(\"Start Over\", key=\"clear_conversation\")\n    st.sidebar.expander(\"Session State\").write(st.session_state)\n\n\ndef init_messages():\n\n    # Initialize chat history\n    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    \ndef get_similar_chunks (question):\n\n    cmd = \"\"\"\n        with results as\n        (SELECT RELATIVE_PATH,\n           VECTOR_COSINE_SIMILARITY(docs_chunks_table.chunk_vec,\n                    SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', ?)) as similarity,\n           chunk\n        from docs_chunks_table\n        order by similarity desc\n        limit ?)\n        select chunk, relative_path from results \n    \"\"\"\n    \n    df_chunks = session.sql(cmd, params=[question, num_chunks]).to_pandas()       \n\n    df_chunks_lenght = len(df_chunks) -1\n\n    similar_chunks = \"\"\n    for i in range (0, df_chunks_lenght):\n        similar_chunks += df_chunks._get_value(i, 'CHUNK')\n\n    similar_chunks = similar_chunks.replace(\"'\", \"\")\n             \n    return similar_chunks\n\n\ndef get_chat_history():\n#Get the history from the st.session_stage.messages according to the slide window parameter\n    \n    chat_history = []\n    \n    start_index = max(0, len(st.session_state.messages) - slide_window)\n    for i in range (start_index , len(st.session_state.messages) -1):\n         chat_history.append(st.session_state.messages[i])\n\n    return chat_history\n\n    \ndef summarize_question_with_history(chat_history, question):\n# To get the right context, use the LLM to first summarize the previous conversation\n# This will be used to get embeddings and find similar chunks in the docs for context\n\n    prompt = f\"\"\"\n        Based on the chat history below and the question, generate a query that extend the question\n        with the chat history provided. The query should be in natual language. \n        Answer with only the query. Do not add any explanation.\n        \n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <question>\n        {question}\n        </question>\n        \"\"\"\n    \n    cmd = \"\"\"\n            select snowflake.cortex.complete(?, ?) as response\n          \"\"\"\n    df_response = session.sql(cmd, params=[st.session_state.model_name, prompt]).collect()\n    summary = df_response[0].RESPONSE     \n\n    if st.session_state.debug:\n        st.sidebar.text(\"Summary to be used to find similar chunks in the docs:\")\n        st.sidebar.caption(summary)\n\n    summry = summary.replace(\"'\", \"\")\n\n    return summary\n\ndef create_prompt (myquestion):\n\n    if st.session_state.use_chat_history:\n        chat_history = get_chat_history()\n\n        if chat_history != []: #There is chat_history, so not first question\n            question_summary = summarize_question_with_history(chat_history, myquestion)\n            prompt_context =  get_similar_chunks(question_summary)\n        else:\n            prompt_context = get_similar_chunks(myquestion) #First question when using history\n    else:\n        prompt_context = get_similar_chunks(myquestion)\n        chat_history = \"\"\n  \n    prompt = f\"\"\"\n           You are an expert chat assistance that extracs information from the CONTEXT provided\n           between <context> and </context> tags.\n           You offer a chat experience considering the information included in the CHAT HISTORY\n           provided between <chat_history> and </chat_history> tags..\n           When ansering the question contained between <question> and </question> tags\n           be concise and do not hallucinate. \n           If you donÂ´t have the information just say so.\n           \n           Do not mention the CONTEXT used in your answer.\n           Do not mention the CHAT HISTORY used in your asnwer.\n           \n           <chat_history>\n           {chat_history}\n           </chat_history>\n           <context>          \n           {prompt_context}\n           </context>\n           <question>  \n           {myquestion}\n           </question>\n           Answer: \n           \"\"\"\n\n    return prompt\n\n\ndef complete(myquestion):\n\n    prompt =create_prompt (myquestion)\n    cmd = \"\"\"\n            select snowflake.cortex.complete(?, ?) as response\n          \"\"\"\n    \n    df_response = session.sql(cmd, params=[st.session_state.model_name, prompt]).collect()\n    return df_response\n\nif __name__ == \"__main__\":\n    main()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64b64aa1-1056-464c-b2f8-86da91346af7",
   "metadata": {
    "name": "cell37",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "The app will look like this (note the side bar where you can select several options)"
  },
  {
   "cell_type": "code",
   "id": "e3f5c05d-bf38-45ae-9e36-600521bfff25",
   "metadata": {
    "language": "python",
    "name": "cell61",
    "resultHeight": 616
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/693166995b606fa6.png', width = 750)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7fe40518-d037-44ad-b428-913a7d8b3172",
   "metadata": {
    "name": "cell62",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Before we go deep into the code explanation, you can run a quick test to see how the ChatBot works when using memory (the chat history) or not. This would be the conversation we would expect from any ChatBot:"
  },
  {
   "cell_type": "code",
   "id": "8ebe8eb9-6562-4a72-a5d2-1db58a40083b",
   "metadata": {
    "language": "python",
    "name": "cell63",
    "codeCollapsed": false,
    "resultHeight": 376
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/53467801bcb84f71.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec777c29-75a9-4c7e-a0e0-92e0190617f6",
   "metadata": {
    "name": "cell64",
    "collapsed": false,
    "resultHeight": 159
   },
   "source": "We are asking for the name of the ski boots, and when we ask where have been tested and later what they are good for the ChatBoot understand we are talking about the ski boots.\n\nFor educational purposes (because this is not something we will implement when creating the ChatBot) we have enabled a check \"Do you want that I remember the chat history?\" that when disabled, the app will not use memory. So each question will be independent from the previous ones. This is what happens with the same questions:"
  },
  {
   "cell_type": "code",
   "id": "da716da3-7bef-4f6e-9c3a-0dd22eef9347",
   "metadata": {
    "language": "python",
    "name": "cell65",
    "codeCollapsed": false,
    "resultHeight": 71
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/40c6da601f5567ee.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7a0828f9-4ed9-4e87-b29b-79973d018e7f",
   "metadata": {
    "name": "cell66",
    "collapsed": false,
    "resultHeight": 1985
   },
   "source": "Basically, each question is answered individually. The second question loses the context of ski boots and looks for what has been tested in our documents. The third one, just find references to what are good for, but with no context.\n\n### Code Explanation\n\n    def get_similar_chunks (question):\n        return\n\nGiven a question, this function is going to calculate its embeddings and look for the most similar chunks within the table. Will return those chunks of text with the highest similarity.\n\n    def get_chat_history():\n        return\n\nThis function is going to return the previous conversation in the chat up to a limit defined by the global variable 'slide_window'. We take advantage of:\n\n    st.session_state.messages\n\nEverytime a question is asked is stored in the state with this code:\n\n    st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n\nAnd the responses provided are also stored:\n\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": res_text})\n\nTherefore 'st.session_state.messages' is used as the chat memory\n\nThis next function 'summarize_question_with_history()' takes the chat_history that we have got previously and the new question being asked. Remember that we are using the information from the PDF documents to answer questions. Therefore, we need to identify using vectors and cosine_similarity what chunk of text will be relevant to answer the question.\n\nTherefore, if we just use \"where have been tested\" sentence to look for the similar vector, probably we will not get the right one. We need to inclue more information in the call we make to 'get_similar_chunks()'.\n\nTake a look at this 'summarize_question_with_history()' function:\n\n    def summarize_question_with_history(chat_history, question):\n    # To get the right context, use the LLM to first summarize the previous conversation\n    # This will be used to get embeddings and find similar chunks in the docs for context\n\n    prompt = f\"\"\"\n        Based on the chat history below and the question, generate a query that extend the question\n        with the chat history provided. The query should be in natual language. \n        Answer with only the query. Do not add any explanation.\n        \n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <question>\n        {question}\n        </question>\n        \"\"\"\n    \n    cmd = \"\"\"\n            select snowflake.cortex.complete(?, ?) as response\n          \"\"\"\n    df_response = session.sql(cmd, params=[st.session_state.model_name, prompt]).collect()\n    summary = df_response[0].RESPONSE     \n\n    if st.session_state.debug:\n        st.sidebar.text(\"Summary to be used to find similar chunks in the docs:\")\n        st.sidebar.caption(sumary)\n\n    summary = summary.replace(\"'\", \"\")\n\n    return summary\n\nIn that function we are using the 'complete()' function where we have created a specific prompt to provide a chat history and the new question in order to get the query that will be used to find the right context.\n\nIn order to experiment with this, we have also created a selection check \"Debug: Click to see summary generated of previous conversation\" that will allow you to see what is the question being used to look for similar chunks.\n\nIn the example provided above, this will be the question used:"
  },
  {
   "cell_type": "code",
   "id": "1a5d4eed-bc43-4397-adef-c7f64f93374c",
   "metadata": {
    "language": "python",
    "name": "cell77",
    "codeCollapsed": false,
    "resultHeight": 362
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/da0925b91a913f8c.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d279bf47-59fd-4839-9d7a-59269fb7bdcf",
   "metadata": {
    "name": "cell78",
    "collapsed": false,
    "resultHeight": 1605
   },
   "source": "So the question that is being vectorized and used to look for similarities is \"What is the place where the TDBootz Special ski boots have been tested?\" that will provide better results than just \"Where have been tested\" as in the case of not using memory in the chat.\n\nNow the function:\n\n    def create_prompt (myquestion):\n        return\n\nUse all those previous functions to get the 'chat_history' and 'prompt_context' and build the prompt. So, this prompt is adding the previous conversation plus the context extracted from the PDFs and the new question.\n\n    prompt = f\"\"\"\n           You are an expert chat assistance that extracs information from the CONTEXT provided\n           between <context> and </context> tags.\n           You offer a chat experience considering the information included in the CHAT HISTORY\n           provided between <chat_history> and </chat_history> tags..\n           When ansering the question contained between <question> and </question> tags\n           be concise and do not hallucinate. \n           If you donÂ´t have the information just say so.\n           \n           Do not mention the CONTEXT used in your answer.\n           Do not mention the CHAT HISTORY used in your asnwer.\n           \n           <chat_history>\n           {chat_history}\n           </chat_history>\n           <context>          \n           {prompt_context}\n           </context>\n           <question>  \n           {myquestion}\n           </question>\n           Answer: \n           \"\"\"\n\nWhat makes this very easy with Streamlit are [st.chat_input](https://docs.streamlit.io/develop/api-reference/chat/st.chat_input?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O) and [st.chat_message](https://docs.streamlit.io/develop/api-reference/chat/st.chat_message?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O). This is the code that get the question with st.chat_input, add it to st.chat_message and to the memmory with st.session_state.messages.append and call 'complete()' function to get the answer that is also printed and stored:\n\n    if question := st.chat_input(\"What do you want to know about your products\"): \n        # Add user message to chat history\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n        # Display user message in chat message container\n        with st.chat_message(\"user\"):\n            st.markdown(question)\n        # Display assistant response in chat message container\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()\n    \n            question = question.replace(\"'\",\"\")\n    \n            with st.spinner(f\"{st.session_state.model_name} thinking...\"):\n                response = complete(question)\n                res_text = response[0].RESPONSE     \n            \n                res_text = res_text.replace(\"'\", \"\")\n                message_placeholder.markdown(res_text)\n        \n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": res_text})\n\nHere you have other examples of chats using info from your documents:"
  },
  {
   "cell_type": "code",
   "id": "3eda5810-55fb-4e99-954c-7f10fabad7a0",
   "metadata": {
    "language": "python",
    "name": "cell85",
    "codeCollapsed": false,
    "resultHeight": 501
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/75dd53d6ea37c9e5.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "572b82a2-3471-47e5-8fdf-7ea2974353b4",
   "metadata": {
    "name": "cell86",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Another example:"
  },
  {
   "cell_type": "code",
   "id": "7f401198-6f43-4c38-9ab9-d96ba86463d7",
   "metadata": {
    "language": "python",
    "name": "cell87",
    "resultHeight": 421
   },
   "outputs": [],
   "source": "st.image('https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/img/4757eed06b1e91ed.png', width = 500)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "452434c8-5004-49c4-8651-27c3f032c996",
   "metadata": {
    "name": "cell88",
    "collapsed": false,
    "resultHeight": 211
   },
   "source": "Here another suggestion based on specific info from the documents (unique for us):\n\n- What is the name of the ski boots?\n- Where have been tested?\n- Who tested them?\n- Do they include any special component?\n\nYou can try with your own documents. You will notice different peformance depending on the LLM you will be using."
  },
  {
   "cell_type": "markdown",
   "id": "796a48f3-33f8-474f-a847-d3d8e76fa5d0",
   "metadata": {
    "name": "cell89",
    "collapsed": false,
    "resultHeight": 271
   },
   "source": "## 6. Optional: Automatic Processing of New Documents\n\nWe can use Snowflake features Streams and Task to automatically process new PDF files as they are added into Snowflake.\n\n- First we are creating a Snowflake Task. That Task will have some conditions to be executed and one action to take:\n    - Where: This is going to be executed using warehouse XS_WH. Please name to your own Warehouse.\n    - When: Check every minute, and execute in the case of new records in the docs_stream stream\n    - What to do: Process the files and insert the records in the docs_chunks_table\n    \nExecute this code in your Snowflake worksheet:"
  },
  {
   "cell_type": "code",
   "id": "73ab2400-064d-432f-ba7d-c85e620aa60e",
   "metadata": {
    "language": "sql",
    "name": "cell91",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "create or replace stream docs_stream on stage docs;\n\ncreate or replace task task_extract_chunk_vec_from_pdf \n    warehouse = COMPUTE_WH\n    schedule = '1 minute'\n    when system$stream_has_data('docs_stream')\n    as\n\n    INSERT INTO docs_chunks_table (relative_path, size, file_url, chunk, chunk_vec)\n    SELECT\n        relative_path, \n        size,\n        file_url, \n        split_table.chunks AS chunk,\n        SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', chunk[0]) AS chunk_vec\n    FROM (\n        SELECT relative_path, size, file_url, SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(parsed_table.parsed:content, 'none', 4000, 400) AS chunks\n        FROM (\n            SELECT relative_path, size, file_url, SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@docs', relative_path, { 'mode': 'OCR' }) as parsed\n            FROM directory(@docs)\n        ) AS parsed_table\n    ) as func;\n\nalter task task_extract_chunk_vec_from_pdf resume;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74765cbe-78f3-4290-8c66-e8543cb0fbbc",
   "metadata": {
    "name": "cell90",
    "collapsed": false,
    "resultHeight": 175
   },
   "source": "You can add a new PDF document and check that in around a minute, it will be available to be used within your Streamlit application. You may want to upload your own documents or try with this new bike guide:\n\n- The Xtreme Road Bike 105 SL\n\nAfter uploading the document (and if you are fast enough before the doc is automatically processed) you can see the doc in the stream:"
  },
  {
   "cell_type": "code",
   "id": "32032af4-a68c-47a5-8ba9-0b402242bc0a",
   "metadata": {
    "language": "sql",
    "name": "cell92",
    "codeCollapsed": false,
    "resultHeight": 146
   },
   "outputs": [],
   "source": "select * from docs_stream;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a005617a-79a4-40f3-878d-ec1633c1c3a7",
   "metadata": {
    "name": "cell93",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "It will return no value once the doc has been processed. Once the documet is in your table, you can start asking questions and will be used in the RAG Process"
  },
  {
   "cell_type": "code",
   "id": "b4de5734-6cac-4ac5-bec4-13581d0da656",
   "metadata": {
    "language": "sql",
    "name": "cell94",
    "codeCollapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "select * from docs_chunks_table where relative_path ilike '%Road_Bike%';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "edf77b27-6fcd-4b09-b668-6e4b5d90b6fa",
   "metadata": {
    "name": "cell95",
    "collapsed": false,
    "resultHeight": 195
   },
   "source": "Try asking questions that are unique in that new bike guide like:\n\n- What tires model brings the road bike?\n- Is there any discount for the Xtreme Road Bike?\n\n(note: try different models to see different results)\n\nOnce you have finish testing uploading new documents and asking questions, you may want to suspend the task:"
  },
  {
   "cell_type": "code",
   "id": "5e7ca0ba-6043-4235-b2b4-301ac931722e",
   "metadata": {
    "language": "sql",
    "name": "cell96",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "alter task task_extract_chunk_vec_from_pdf suspend;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27306cfe-ce16-4874-b9de-4bc3f97f02c4",
   "metadata": {
    "name": "cell97",
    "collapsed": false,
    "resultHeight": 595
   },
   "source": "## 7. Conclusion & Resources\n\nCongratulations! You've successfully performed RAG using Snowflake Cortex and securely built a full-stack RAG application in Snowflake without having to build integrations, manage any infrastructure or deal with security concerns with data moving outside of the Snowflake governance framework.\n\n### What You Learned\n\n- Creating functions to automatically extract text and chunk PDF files\n- Creating embeddings with Snowflake Cortex\n- Use Snowflake VECTOR data type for similarity search\n- Using Snowflake Cortex to use LLMs to answer questions\n- Building an application front-end with Streamlit\n- Using directory tables and streams with task to automatically process files\n\n### Related Resources\n\n- [Docs: Snowflake Cortex LLM functions](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)\n- [Blog: RAG explainer and how it works in Snowflake](https://www.snowflake.com/blog/easy-secure-llm-inference-retrieval-augmented-generation-rag-cortex/?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)\n- [Blog: Customizing RAG framework components using Snowpark Container Services](https://medium.com/snowflake/developing-a-product-chatbot-with-airmiles-in-snowflake-6b197d3fc424)\n- [Docs: Snowflake Directory Tables](https://docs.snowflake.com/en/user-guide/data-load-dirtables?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)\n- [Docs: Creating User-Defined Table Functions](https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-udtfs?_fsi=P4Dy8V0O&_fsi=P4Dy8V0O)"
  },
  {
   "cell_type": "markdown",
   "id": "136a7a8b-9704-43cc-9e4a-862ad025bd9e",
   "metadata": {
    "name": "cell17",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Lastly, we can drop all of the resouces we have created for this lab."
  },
  {
   "cell_type": "code",
   "id": "41f590df-093c-44fc-bd25-f656d35bb083",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "drop database CORTEX_HOL_DB;\ndrop database HOL_CORTEX_DOCS;",
   "execution_count": null
  }
 ]
}